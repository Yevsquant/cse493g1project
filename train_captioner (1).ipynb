{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jVv0661IKTVY",
        "outputId": "a7f3c2ff-06d4-4622-ca59-6ba6c4e751c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cse493g1/assignments/assignment2/'\n",
        "FOLDERNAME = 'cse493g1/cse493g1project/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model_trainer import Trainer\n",
        "from Model import GraphCaptioningModel\n",
        "from model_utils import decode_captions, create_minibatch, encode_captions"
      ],
      "metadata": {
        "id": "gY0E-OtnKU7m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "\n",
        "class GraphImageDataset(Dataset):\n",
        "    def __init__(self, csv_files, transform=None):\n",
        "        self.data = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x, y = self.data.iloc[idx]\n",
        "      x_out = str(x)\n",
        "      y_out = str(y)\n",
        "      return x_out, y_out"
      ],
      "metadata": {
        "id": "GoU4ONJVUtaa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files2 = ['/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_kk1.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_cr1.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_gv1.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_sp1.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_kk1_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_cr1_medium.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_gv1_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_sp1_medium.csv']\n",
        "\n",
        "dataset_mixed = GraphImageDataset(csv_files=csv_files2)"
      ],
      "metadata": {
        "id": "ZrSKCyKRUvIp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as transform\n",
        "from pathlib import Path\n",
        "\n",
        "raw_data_clr = {}\n",
        "clr_len = dataset_mixed.__len__()\n",
        "\n",
        "graph_list = []\n",
        "caption_list = []\n",
        "for i in np.random.choice(clr_len, 600):\n",
        "  graph_path, caption = dataset_mixed.__getitem__(i)\n",
        "  graph = F.pil_to_tensor(PIL.Image.open('/content/drive/My Drive/cse493g1/cse493g1project/datasets' + graph_path).convert('RGB'))\n",
        "  graph_list.append(np.array([graph.numpy()]).reshape(graph.shape))\n",
        "  caption_list.append(caption)\n",
        "raw_data_clr['features'] = np.array(graph_list)\n",
        "raw_data_clr['captions'] = np.array(caption_list)"
      ],
      "metadata": {
        "id": "nRRE7QDpUxRy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from model_trainer import Trainer\n",
        "from Model import GraphCaptioningModel\n",
        "from model_utils import decode_captions, create_minibatch, encode_captions\n",
        "\n",
        "torch.manual_seed(493)\n",
        "np.random.seed(493)\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "data_clr = {}\n",
        "\n",
        "data_clr['idx_to_word'] = ['<NULL>', '<START>', '<END>']\n",
        "for i in range(100):\n",
        "  data_clr['idx_to_word'].append(str(i))\n",
        "punc = ['{', '}', '[', ']', '(', ')', ':', ',', ' ']\n",
        "for p in punc:\n",
        "  data_clr['idx_to_word'].append(p)\n",
        "\n",
        "data_clr['word_to_idx'] = {}\n",
        "for i in range(len(data_clr['idx_to_word'])):\n",
        "  data_clr['word_to_idx'][data_clr['idx_to_word'][i]] = i\n",
        "\n",
        "\n",
        "encoded_captions = encode_captions(raw_data_clr['captions'][:600], data_clr['word_to_idx'])\n",
        "features = raw_data_clr['features'][:600]\n",
        "\n",
        "data_clr['train_captions'] = encoded_captions[:500]\n",
        "data_clr['train_features'] = features[:500]\n",
        "\n",
        "data_clr['val_captions'] = encoded_captions[500:600]\n",
        "data_clr['val_features'] = features[500:600]\n",
        "\n",
        "data_clr['test_captions'] = encoded_captions\n",
        "data_clr['test_features'] = features\n",
        "\n",
        "graph_model_clr = GraphCaptioningModel(\n",
        "          word_to_idx=data_clr['word_to_idx'],\n",
        "          wordvec_dim=256,\n",
        "          max_length=600\n",
        "        )\n",
        "\n",
        "model_solver_clr = Trainer(graph_model_clr, data_clr, idx_to_word=data_clr['idx_to_word'],\n",
        "          num_epochs=5,\n",
        "          batch_size=3,\n",
        "          learning_rate=0.0005,\n",
        "          verbose=True, print_every=10,\n",
        "        )\n",
        "\n",
        "model_solver_clr.train()\n",
        "\n",
        "# Plot the training losses.\n",
        "plt.plot(model_solver_clr.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eOniQKrwpyj",
        "outputId": "a9326bf2-5bc3-4705-9383-863e0f021f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 147MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 830) loss: 4.974954\n",
            "(Iteration 11 / 830) loss: 2.453613\n",
            "(Iteration 21 / 830) loss: 2.686307\n",
            "(Iteration 31 / 830) loss: 1.896051\n",
            "(Iteration 41 / 830) loss: 2.170106\n",
            "(Iteration 51 / 830) loss: 2.573402\n",
            "(Iteration 61 / 830) loss: 2.008980\n",
            "(Iteration 71 / 830) loss: 2.137163\n",
            "(Iteration 81 / 830) loss: 2.389022\n",
            "(Iteration 91 / 830) loss: 1.762740\n",
            "(Iteration 101 / 830) loss: 2.171813\n",
            "(Iteration 111 / 830) loss: 2.256003\n",
            "(Iteration 121 / 830) loss: 2.287055\n",
            "(Iteration 131 / 830) loss: 2.147652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in ['val', 'train']:\n",
        "    minibatch = create_minibatch(data_clr, split=split, batch_size=1)\n",
        "    gt_captions, features = minibatch\n",
        "    gt_captions = decode_captions(gt_captions.astype(int), data_clr['idx_to_word'])\n",
        "\n",
        "    sample_captions = graph_model_clr.sample(features, max_length=100)\n",
        "    sample_captions = decode_captions(sample_captions, data_clr['idx_to_word'])\n",
        "\n",
        "    for gt_caption, sample_caption, features in zip(gt_captions, sample_captions, features):\n",
        "        # Skip missing URLs.\n",
        "        plt.imshow(features.T)\n",
        "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "KfzB0L7CU8KY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}