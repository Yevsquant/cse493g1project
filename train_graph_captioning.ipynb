{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOq2mqIWNu-",
        "outputId": "85c35e08-8bd4-4896-b0cb-18dfae00b840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cse493g1/assignments/assignment2/'\n",
        "FOLDERNAME = 'cse493g1/cse493g1project/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model_trainer import Trainer\n",
        "from Model import GraphCaptioningModel\n",
        "from model_utils import decode_captions, create_minibatch, encode_captions"
      ],
      "metadata": {
        "id": "nsgDVfpFbOcw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "\n",
        "class GraphImageDataset(Dataset):\n",
        "    def __init__(self, csv_files, transform=None):\n",
        "        self.data = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x, y = self.data.iloc[idx]\n",
        "      x_out = str(x)\n",
        "      y_out = str(y)\n",
        "      return x_out, y_out\n",
        "\n",
        "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "metadata": {
        "id": "YDZOK295369h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files1 = ['/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_kk0.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_cr0.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_gv0.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_sp0.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_kk0_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_cr0_medium.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_gv0_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_sp0_medium.csv']\n",
        "\n",
        "csv_files2 = ['/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_kk0.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_cr0.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_gv0.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_sp0.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_kk0_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_cr0_medium.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_gv0_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_sp0_medium.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_kk1.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_cr1.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_gv1.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_small/data_sp1.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_kk1_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_cr1_medium.csv',\n",
        "              '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_gv1_medium.csv', '/content/drive/My Drive/cse493g1/cse493g1project/datasets/datasets_medium/data_sp1_medium.csv']\n",
        "dataset_default = GraphImageDataset(csv_files=csv_files1)\n",
        "dataset_mixed = GraphImageDataset(csv_files=csv_files2)"
      ],
      "metadata": {
        "id": "ALLxNpY11XL7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_mixed.__getitem__(2500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2mSp-ViLZ4m",
        "outputId": "ceff09f6-e926-43c8-a9a5-90639019c736"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('/datasets_small/graph_images_sp_color/sp1_gi20.png', '{16: [(16, 17), (16, 19), (16, 48), (16, 84), (16, 98)], 17: [(16, 17), (17, 46), (17, 48), (17, 59), (17, 80), (17, 84), (17, 98)], 19: [(16, 19), (19, 38), (19, 43), (19, 46), (19, 48), (19, 59), (19, 80), (19, 84), (19, 98)], 38: [(19, 38), (38, 38), (38, 43), (38, 46), (38, 48), (38, 59), (38, 84), (38, 98)], 43: [(19, 43), (38, 43), (43, 46), (43, 59), (43, 80), (43, 98)], 46: [(17, 46), (19, 46), (38, 46), (43, 46), (46, 48), (46, 59), (46, 80), (46, 84), (46, 98)], 48: [(16, 48), (17, 48), (19, 48), (38, 48), (46, 48), (48, 59), (48, 80), (48, 98)], 59: [(17, 59), (19, 59), (38, 59), (43, 59), (46, 59), (48, 59), (59, 80)], 80: [(17, 80), (19, 80), (43, 80), (46, 80), (48, 80), (59, 80)], 84: [(16, 84), (17, 84), (19, 84), (38, 84), (46, 84)], 98: [(16, 98), (17, 98), (19, 98), (38, 98), (43, 98), (46, 98), (48, 98), (98, 98)]}')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as transform\n",
        "from pathlib import Path\n",
        "\n",
        "raw_data_nc = {}\n",
        "nc_len = dataset_default.__len__()\n",
        "raw_data_clr = {}\n",
        "clr_len = dataset_mixed.__len__()\n",
        "\n",
        "\"\"\"\n",
        "graph_list = []\n",
        "caption_list = []\n",
        "for i in range(nc_len):\n",
        "  graph_path, caption = dataset_default.__getitem__(i)\n",
        "  graph = F.pil_to_tensor(PIL.Image.open('/content/drive/My Drive/cse493g1/cse493g1project/datasets' + graph_path).convert('RGB'))\n",
        "  graph_list.append(np.array([graph.numpy()]).reshape(graph.shape))\n",
        "  caption_list.append(caption)\n",
        "raw_data_nc['features'] = np.array(graph_list)\n",
        "raw_data_nc['captions'] = np.array(caption_list)\n",
        "\"\"\"\n",
        "\n",
        "graph_list = []\n",
        "caption_list = []\n",
        "for i in np.random.choice(4640, 3000):\n",
        "  graph_path, caption = dataset_mixed.__getitem__(i)\n",
        "  graph = F.pil_to_tensor(PIL.Image.open('/content/drive/My Drive/cse493g1/cse493g1project/datasets' + graph_path).convert('RGB'))\n",
        "  graph_list.append(np.array([graph.numpy()]).reshape(graph.shape))\n",
        "  caption_list.append(caption)\n",
        "raw_data_clr['features'] = np.array(graph_list)\n",
        "raw_data_clr['captions'] = np.array(caption_list)\n",
        "\n",
        "\"\"\"\n",
        "graph1 = F.pil_to_tensor(PIL.Image.open('/content/drive/My Drive/cse493g1/cse493g1project/gi0.png').convert('RGB'))\n",
        "#graph2 = F.pil_to_tensor(PIL.Image.open('gi1.png').convert('RGB'))\n",
        "#graph3 = F.pil_to_tensor(PIL.Image.open('gi3.png').convert('RGB'))\n",
        "raw_data['features'] = np.array([graph1.numpy()]).reshape(graph1.shape)\n",
        "raw_data['features'] = raw_data['features'][:,:512,:512]\n",
        "raw_data['captions'] = np.array([\"{74: [(85, 74), (74, 65), (32, 74), (79, 74)], 32: [(27, 32), (32, 85), (32, 78), (32, 74), (32, 78)], 45: [(79, 45), (45, 45), (45, 33)], 64: [(79, 64), (30, 64), (43, 64)], 78: [(65, 78), (85, 78), (32, 78), (32, 78), (65, 78), (78, 33)], 85: [(85, 74), (85, 79), (32, 85), (85, 78), (85, 27), (33, 85)], 43: [(43, 43), (43, 64), (65, 43)], 63: [], 5: [(79, 5), (79, 5), (36, 5)], 33: [(33, 79), (45, 33), (78, 33), (38, 33), (33, 72), (33, 85)], 27: [(27, 32), (27, 72), (27, 79), (85, 27)], 79: [(79, 45), (79, 64), (85, 79), (33, 79), (79, 5), (72, 79), (72, 79), (79, 74), (79, 36), (79, 5), (27, 79)], 38: [(38, 65), (36, 38), (38, 33)], 36: [(36, 38), (79, 36), (72, 36), (36, 5)], 30: [(30, 64)], 72: [(27, 72), (72, 79), (72, 79), (72, 36), (33, 72)], 65: [(38, 65), (74, 65), (65, 78), (65, 65), (65, 78), (65, 43)]}\"])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wfrHKcsSCBiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "print(raw_data_nc['features'].shape)\n",
        "print(raw_data_nc['features'][0])\n",
        "print(raw_data_nc['features'][0].shape)\n",
        "print(raw_data_nc['captions'].shape)\n",
        "print(raw_data_nc['captions'][0])\n",
        "print(raw_data_nc['captions'][0].shape)\n",
        "\"\"\"\n",
        "print(\"MIXED COLOR DATA\")\n",
        "\n",
        "print(raw_data_clr['features'].shape)\n",
        "print(raw_data_clr['features'][0])\n",
        "print(raw_data_clr['features'][0].shape)\n",
        "print(raw_data_clr['captions'].shape)\n",
        "print(raw_data_clr['captions'][0])\n",
        "print(raw_data_clr['captions'][0].shape)"
      ],
      "metadata": {
        "id": "oyJ7J3u5IHYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Model import GraphCaptioningModel\n",
        "data = {}\n",
        "\n",
        "data['idx_to_word'] = ['<NULL>', '<START>', '<END>']\n",
        "for i in range(100):\n",
        "  data['idx_to_word'].append(str(i))\n",
        "punc = ['{', '}', '[', ']', '(', ')', ':', ',', ' ']\n",
        "for p in punc:\n",
        "  data['idx_to_word'].append(p)\n",
        "\n",
        "data['word_to_idx'] = {}\n",
        "for i in range(len(data['idx_to_word'])):\n",
        "  data['word_to_idx'][data['idx_to_word'][i]] = i\n",
        "\n",
        "data['train_captions'] = torch.tensor(encode_captions(raw_data['captions'], data['word_to_idx'])).type(dtype)\n",
        "data['train_features'] = torch.tensor(np.array([raw_data['features']])).type(dtype)\n",
        "print(data['train_features'].shape)\n",
        "print(data['train_captions'].shape)\n",
        "\n",
        "transformer = GraphCaptioningModel(\n",
        "          word_to_idx=data['word_to_idx'],\n",
        "          wordvec_dim=256,\n",
        "          max_length=2000\n",
        "        ).type(dtype)\n",
        "\n",
        "\n",
        "transformer_solver = Trainer(transformer, data, idx_to_word=data['idx_to_word'],\n",
        "           num_epochs=10,\n",
        "           batch_size=1,\n",
        "           learning_rate=0.001,\n",
        "           verbose=True, print_every=10,\n",
        "         )\n",
        "\n",
        "transformer_solver.train()\n",
        "\n",
        "# Plot the training losses.\n",
        "plt.plot(transformer_solver.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SMcZiFa2IQSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "62a37102-a030-4fb3-e8e5-d78a19365de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fe37e7ae1e50>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_to_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx_to_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_captions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'captions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_to_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Model import GraphCaptioningModel\n",
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "data_clr = {}\n",
        "\n",
        "data_clr['idx_to_word'] = ['<NULL>', '<START>', '<END>']\n",
        "for i in range(100):\n",
        "  data_clr['idx_to_word'].append(str(i))\n",
        "punc = ['{', '}', '[', ']', '(', ')', ':', ',', ' ']\n",
        "for p in punc:\n",
        "  data_clr['idx_to_word'].append(p)\n",
        "\n",
        "data_clr['word_to_idx'] = {}\n",
        "for i in range(len(data_clr['idx_to_word'])):\n",
        "  data_clr['word_to_idx'][data_clr['idx_to_word'][i]] = i\n",
        "\n",
        "tenth = len(raw_data_clr)//10\n",
        "\n",
        "encoded_captions = encode_captions(raw_data_clr['captions'], data_clr['word_to_idx'])\n",
        "\n",
        "data_clr['train_captions'] = torch.tensor(encoded_captions[:tenth*8]).type(dtype)\n",
        "data_clr['train_features'] = torch.tensor(raw_data_clr['features'][:tenth*8]).type(dtype)\n",
        "\n",
        "data_clr['val_captions'] = torch.tensor(encoded_captions[tenth*8:tenth*9]).type(dtype)\n",
        "data_clr['val_features'] = torch.tensor(raw_data_clr['features'][tenth*8:tenth*9]).type(dtype)\n",
        "\n",
        "data_clr['test_captions'] = torch.tensor(encoded_captions[tenth*9:]).type(dtype)\n",
        "data_clr['test_features'] = torch.tensor(raw_data_clr['features'][tenth*9:]).type(dtype)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "graph_model_clr = GraphCaptioningModel(\n",
        "          word_to_idx=data['word_to_idx'],\n",
        "          wordvec_dim=256,\n",
        "          max_length=1600\n",
        "        ).type(dtype)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "model_solver_clr = Trainer(graph_model_clr, data_clr, idx_to_word=data['idx_to_word'],\n",
        "           num_epochs=10,\n",
        "           batch_size=10,\n",
        "           learning_rate=0.001,\n",
        "           verbose=True, print_every=10,\n",
        "         )\n",
        "\n",
        "model_solver_clr.train()\n",
        "\n",
        "# Plot the training losses.\n",
        "plt.plot(transformer_solver.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZT_DHS1wauox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in ['train', 'val']:\n",
        "    minibatch = create_minibatch(data, split=split, batch_size=2)\n",
        "    gt_captions, features, urls = minibatch\n",
        "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
        "\n",
        "    sample_captions = transformer.sample(features, max_length=30)\n",
        "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
        "\n",
        "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
        "        # Skip missing URLs.\n",
        "        if img is None: continue\n",
        "        plt.imshow(img)\n",
        "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "BE4-fC4Tavdo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}